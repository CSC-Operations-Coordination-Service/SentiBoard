from flask import Response, request

@app.route('/robots.txt')
def robots_txt():
    host = request.host.lower()

    if "staging.sentiboard.onda-dias.com" in host:
        # Staging environment → block all crawlers
        content = """# robots.txt for Copernicus Sentinel Operations Dashboard (Staging)

User-agent: *
Disallow: /
"""
    else:
        # Production environment → allow crawling
        content = """# robots.txt for Copernicus Sentinel Operations Dashboard (Production)

User-agent: *
# Allow crawling of all main content pages
Allow: /

# Disallow crawling of staging or test paths (if any exist)
Disallow: /staging/
Disallow: /test/
Disallow: /tmp/

# Disallow crawling of static assets (CSS, JS, fonts, images)
Disallow: /static/

# Sitemap reference for better indexing
Sitemap: https://operations.dashboard.copernicus.eu/sitemap.xml
"""

    return Response(content, mimetype='text/plain')
